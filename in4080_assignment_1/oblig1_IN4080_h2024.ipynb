{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN4080: obligatory assignment 1 (Autumn 2024)\n",
    " \n",
    "Mandatory assignment 1 consists of three parts. In Part 1 (6 points), you will test and improve on a BPE (Byte-Pair-Encoding) tokenizer . In Part 2 (7 points), you will estimate an N-gram language model, based on a training corpus and the tokenizer you worked on in Part 1. Finally, in Part 3 (7 points), you will develop a basic classification model to distinguish between Bokmål and Nynorsk sentences.\n",
    "\n",
    "You should answer all three parts. You are required to get at least 12/20 points to pass. The most important is that you try to answer each question (possibly with some mistakes), to help you gain a better and more concrete understanding of the topics covered during the lectures. There are also bonus questions for those of you who would like to deepen their understanding of the topics covered by this assignment.\n",
    "\n",
    "- We assume that you have read and are familiar with IFI’s requirements and guidelines for mandatory assignments, see [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html) and [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-guidelines.html).\n",
    "- This is an individual assignment. You should not deliver joint submissions. \n",
    "- You may redeliver in Devilry before the deadline (__Sunday, September 15 at 23:59__), but include all files in the last delivery.\n",
    "- Only the last delivery will be read! If you deliver more than one file, put them into a zip-archive. You don't have to include in your delivery the files already provided for this assignment. \n",
    "- Name your submission _your\\_username\\_in4080\\_mandatory\\_1_\n",
    "- You can work on this assignment either on the IFI machines or on your own computer. \n",
    "\n",
    "*The preferred format for the assignment is a completed version of this Jupyter notebook*, containing both your code and explanations about the steps you followed. We want to stress that simply submitting code is __not__ by itself sufficient to complete the assignment - we expect the notebook to also contain explanations of what you have implemented, along with motivations for the choices you made along the way. Preferably use whole sentences, and mathematical formulas if necessary. Explaining in your own words (using concepts we have covered through in the lectures) what you have done and reflecting on your solution is an important part of the learning process - take it seriously!\n",
    "\n",
    "Regarding the use of LLMs (ChatGPT or similar): you are allowed to use them as 'sparring partner', for instance to clarify something you have not understood. However, you are __not__ allowed to use them to generate solutions (either in part or in full) to the assignment tasks. \n",
    "\n",
    "__Technical tip__: Some of the tasks in this assignment will require you to extend methods in classes that are already partly implemented. To implement those methods directly in a Jupyter notebook, you can use the function `setattr` to attach a method to a given class: \n",
    "\n",
    "```python\n",
    "class A:\n",
    "    pass\n",
    "a = A()\n",
    "\n",
    "def foo(self):\n",
    "    print('hello world!')\n",
    "    \n",
    "setattr(A, 'foo', foo)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Tokenisation\n",
    "\n",
    "We will start by building a basic tokenizer relying on white space and punctuation. \n",
    "\n",
    "__Task 1.1__ (2 points): Implement the method `split` below such that it takes a text as input and outputs a list of tokens. The tokenisation should simply be done by splitting on white space, except for punctuation markers and other symbols (`.,:;!?-()\"`), which should correspond to their own token. For instance, the sentence \"Pierre, who works at NR, also teaches at UiO.\" should be split into 12 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " ',',\n",
       " 'who',\n",
       " 'works',\n",
       " 'at',\n",
       " 'NR',\n",
       " ',',\n",
       " 'also',\n",
       " 'teaches',\n",
       " 'at',\n",
       " 'UiO',\n",
       " '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def basic_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"The method should split the text on white space, except for punctuation\n",
    "    markers that should be considered as tokens of their own (even in the \n",
    "    absence of white space before or after their occurrence)\"\"\"\n",
    "\n",
    "    # \\s+    : Matches any whitespace sequence (spaces, tabs, newlines).\n",
    "    # |      : OR operator to match either whitespace or the following group.\n",
    "    # ([\\.,:;!\\?\\-\\(\\)\\\"\"]) : A capturing group for punctuation marks. \n",
    "    # This ensures punctuation is captured and included in the split result.\n",
    "    # \\      : using it before special characters allows us to treat them literally.\n",
    "    tokens = re.split(r'\\s+|([\\.,:;!\\?\\-\\(\\)\\\"\"])', text)\n",
    "\n",
    "    # Filtering out any empty strings resulting from consecutive matches or splitting at the start/end\n",
    "    tokens = [token for token in tokens if token]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "basic_tokenize('Pierre, who works at NR, also teaches at UiO.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop the above code, I consulted the documentation for the Regex library, specifically following the instructions on the following page regarding the use of the split method: https://docs.python.org/3/library/re.html#making-a-phonebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the tokeniser on a small corpus, the [Norwegian Dependency Treebank](https://www.nb.no/sprakbanken/en/resource-catalogue/oai-nb-no-sbr-10/) (the corpus has been annotated with morphological features, syntactic functions and hierarchical structures, but we'll simply use here the raw text and discard all the annotation layers). We provide you with the data in the files `ndt_train_lm.txt` and `ndt_test_lm.txt`. \n",
    "\n",
    "__Task 1.2__ (1 point): Run the tokenizer you have implemented on `ndt_test_lm.txt`. How many tokens were extracted? And how many types (distinct words) were there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First thirty tokens: ['Lam', 'og', 'piggvar', 'på', 'bryllupsmenyen', '|', 'Kamskjell', ',', 'piggvar', 'og', 'lammefilet', 'sto', 'på', 'menyen', 'under', 'den', 'kongelige', 'gallamiddagen', '.', 'Og', 'til', 'dessert', ':', 'Parfait', 'à', 'la', 'Mette', '-', 'Marit', '.']\n",
      "Number of tokens extracted: 259220\n",
      "Number of types: 27586\n"
     ]
    }
   ],
   "source": [
    "file_path = 'ndt_test_lm.txt'\n",
    "\n",
    "with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "tokens = basic_tokenize(text)\n",
    "\n",
    "# Proof that the algorithm actually works by printing the first 30 elements of the list\n",
    "print(f'First thirty tokens: {tokens[:30]}')\n",
    "\n",
    "# Number of tokens extracted\n",
    "print(f'Number of tokens extracted: {len(tokens)}')\n",
    "\n",
    "# Number of unique words (types), not considering punctuation\n",
    "def count_types(a_list):\n",
    "    lower_case_list = [token.lower() for token in a_list] # We turn every word into lowercase \n",
    "                                                          # to avoid any possible repetition in the types.\n",
    "    punctuation_string = '.,:;!?-()\"'\n",
    "    punctuation_set = set(punctuation_string) # Creating a set with all the separators\n",
    "    types = set()\n",
    "    for element in lower_case_list:\n",
    "        if element not in punctuation_set:\n",
    "            types.add(element)  # Adding only unique non-punctuation elements, given that types is a set\n",
    "    return len(types)\n",
    "print(f'Number of types: {count_types(tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now use Byte-Pair Encoding (BPE) to limit the vocabulary of the tokenizer to 5,000.  An initial implementation of the algorithm is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Iterator\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer based on the Byte-Pair Encoding algorithm.\n",
    "    Note: the current implementation is limited to Latin characters (ISO-8859-1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_corpus_file: str, vocab_size = 5000):\n",
    "        \"\"\"\n",
    "        Creates a new BPE tokenizer, with merge pairs found using the given\n",
    "        corpus file. The extraction of merge pairs stops when a vocabulary of \n",
    "        size vocab_size is reached.\n",
    "        \"\"\"\n",
    "\n",
    "        # List of string pairs that should be merged when tokenizing\n",
    "        # Example: ('e', 't'), which means that 'et' is a possible subword\n",
    "        # Each string pair is mapped to an unique index number\n",
    "        # (corresponding to their position in the self.vocab list)\n",
    "        self.merge_pairs = {}\n",
    "\n",
    "        # We add as basic vocab all characters of the extended ASCII\n",
    "        self.vocab = [chr(i) for i in range(256)]\n",
    "\n",
    "        # We specified which encoding to use when reading the file\n",
    "        with open(train_corpus_file, encoding='iso-8859-1') as fd: \n",
    "            # We first read the corpus, split on white space, \n",
    "            # and count the occurrences of each distinct word\n",
    "            print(\"Counting word occurrences in corpus %s\"%train_corpus_file, end=\"...\", flush=True)\n",
    "            text = fd.read()\n",
    "            vocabulary_counts = {}\n",
    "            for token in text.split():\n",
    "                # get() method parameters:\n",
    "                # keyname (required): The keyname of the item we want to return the value from.\n",
    "                # value (pptional): A value to return if the specified key does not exist.\n",
    "                vocabulary_counts[token] = vocabulary_counts.get(token, 0) + 1 # Dictionary {'token':count}\n",
    "            print(\"Done\")\n",
    "\n",
    "            # We then iteratively extend the list of merge pairs \n",
    "            # until we reach the desired size. \n",
    "            # Note: to speed up the algorithm, we extract n merge pairs at each iteration\n",
    "            progress_bar = tqdm(total=vocab_size)\n",
    "            while len(self.vocab) < vocab_size:\n",
    "                most_common_pairs = self.get_most_common_pairs(vocabulary_counts)\n",
    "                for common_pair in most_common_pairs:\n",
    "                    self.merge_pairs[common_pair] = len(self.vocab)\n",
    "                    self.vocab.append(\"\".join(common_pair))\n",
    "                progress_bar.update(len(most_common_pairs))\n",
    "                # print(\"Examples of new subwords:\", [\"\".join(pair) for pair in most_common_pairs][:10])\n",
    "            \n",
    "    def get_most_common_pairs(self, vocabulary_counts: Dict[str,int], n:int=200) -> List[Tuple[str,str]]:\n",
    "        \"\"\"\n",
    "        Given a set of distinct words along with their corresponding number of occurrences in the corpus, \n",
    "        returns the n most frequent pairs of subwords.       \n",
    "        \"\"\"\n",
    "\n",
    "        # We count the frequencies of consecutive subwords in the vocabulary list\n",
    "        pair_freqs = {}\n",
    "        for word, word_count in vocabulary_counts.items():\n",
    "            subwords = self.tokenize_word(word)\n",
    "            for i in range(len(subwords)-1):\n",
    "                byte_pair = (subwords[i], subwords[i+1])\n",
    "                pair_freqs[byte_pair] = pair_freqs.get(byte_pair, 0) + word_count\n",
    "\n",
    "        # And return the most frequent ones\n",
    "        most_freq_pairs = sorted(pair_freqs.keys(), key=lambda x: pair_freqs[x])[::-1][:n]\n",
    "        return most_freq_pairs\n",
    "\n",
    "    def __call__(self, input:str, show_progress_bar=True) -> Iterator[str]:\n",
    "        \"\"\"Tokenizes a full text\"\"\"\n",
    "\n",
    "        # We first split into whitespace-separated tokens, and then in subwords\n",
    "        words = input.split()\n",
    "        for word in tqdm(words) if show_progress_bar else words:\n",
    "            subwords = self.tokenize_word(word)\n",
    "            for subword in subwords:\n",
    "                yield subword\n",
    "                \n",
    "    def tokenize_word(self, word):\n",
    "        \"\"\"Splits the word into subwords, according to the merge pairs \n",
    "        currently stored in self.merge_pairs.\"\"\"\n",
    "\n",
    "        # We start with a list of characters\n",
    "        # (+ a final character to denote the end of the word)    \n",
    "        splits = list(word) + [\" \"]\n",
    "\n",
    "        # We continue until there is nothing left to be merged\n",
    "        while len(splits)>=2:\n",
    "\n",
    "            # We extract consecutive subword pairs\n",
    "            pairs = [(splits[i], splits[i+1]) for i in range(len(splits)-1)]\n",
    "\n",
    "            # We find the \"best\" pair of subwords to merge\n",
    "            # that is, the one with the lowest position in the list of merge rules\n",
    "            best_pair_to_merge = min(pairs, key=lambda x: self.merge_pairs.get(x, np.inf))\n",
    "            if best_pair_to_merge in self.merge_pairs:\n",
    "\n",
    "                # We then merge the two subwords\n",
    "                for i in range(len(splits)-1):\n",
    "                    if (splits[i], splits[i+1]) == best_pair_to_merge:\n",
    "                        merged_subword = self.vocab[self.merge_pairs[best_pair_to_merge]]\n",
    "                        splits = splits[:i] + [merged_subword] + splits[i+2:]\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.3__ (1 point): Learn the BPE tokenizer on the `ndt_train_lm.txt` corpus, and then apply this tokenizer on `ndt_test_lm.txt`. Print the number of tokens and types (distinct subwords) obtained by this tokenizer on the test data. How do those numbers compare to the ones obtained with the basic tokenizer you had implemented earlier ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for training and testing corpora\n",
    "train_file_path = 'ndt_train_lm.txt'\n",
    "test_file_path = 'ndt_test_lm.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting word occurrences in corpus ndt_train_lm.txt...Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f28663c596541f384752301e5b090d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae978d2beb3e47d9a8a0eddc88c73984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 386753\n",
      "Number of types: 4399\n"
     ]
    }
   ],
   "source": [
    "# We train the tokenizer using the `ndt_train_lm.txt` corpus \n",
    "bpe_tokenizer = BPETokenizer(train_corpus_file=train_file_path, vocab_size=5000)\n",
    "\n",
    "# We apply the BPE Tokenizer to the `ndt_test_lm.txt` corpus\n",
    "with open(test_file_path, mode='r', encoding='iso-8859-1') as test_file:\n",
    "    test_text = test_file.read()\n",
    "\n",
    "tokens = list(bpe_tokenizer(test_text))  # Tokenizing the test corpus thanks to the __call__ method\n",
    "\n",
    "# We count the number of tokens and types\n",
    "num_tokens = len(tokens)             # Total number of tokens\n",
    "num_types = len(set(tokens))         # Number of unique tokens (types)\n",
    "\n",
    "print(f\"Number of tokens: {num_tokens}\")\n",
    "print(f\"Number of types: {num_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Basic Tokenizer**\n",
    "\n",
    "Number of tokens: 259 220\n",
    "\n",
    "Number of types: 27 586\n",
    "\n",
    "- **BPE Tokenizer**\n",
    "\n",
    "Number of tokens: 386 753\n",
    "\n",
    "Number of types: 4 399\n",
    "\n",
    "**Final considerations:**\n",
    "\n",
    "The basic tokenizer does not split the single words, while the BPE tokenizer does, thus the highest number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.4__ (2 points): The current BPE implementation is that it treats all characters in the same manner. A rather inconvenient side effect is that letters may be merged together with punctuation markers (like 'ing', ',' --> 'ing,'), if they are not separated by white space. Modify the implementation of the BPE algorithm above to prevent punctuation markers to be merged with letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modified BPE Tokenizer details**: I reported the above implemented 'basic_tokenize' function inside the class, and I modified the initial __init__ function so that when creating the dictionary 'vocabulary_counts', it no longer uses the split method to find the different words (tokens) from the text, but instead uses the 'basic_tokenize' function. Finally, I modified the __call__ function in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- MODIFIED BPE TOKENIZER --------------------------\n",
    "from typing import Dict, List, Tuple, Iterator\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class BPETokenizer_modified:\n",
    "    \"\"\"Tokenizer based on the Byte-Pair Encoding algorithm. \n",
    "    Note: the current implementation is limited to Latin characters (ISO-8859-1)\"\"\"\n",
    "\n",
    "    @staticmethod                                                  # We indicate that the method is independent of any instance or class attributes.\n",
    "    def basic_tokenize(text: str) -> List[str]:\n",
    "        tokens = re.split(r'\\s+|([\\.,:;!\\?\\-\\(\\)\\\"\"])', text)\n",
    "        tokens = [token for token in tokens if token]\n",
    "        return tokens\n",
    "\n",
    "    def __init__(self, train_corpus_file: str, vocab_size = 5000):\n",
    "        \"\"\"Creates a new BPE tokenizer, with merge pairs found using the given\n",
    "        corpus file. The extraction of merge pairs stops when a vocabulary of \n",
    "        size vocab_size is reached.\"\"\"\n",
    "\n",
    "        # List of string pairs that should be merged when tokenizing\n",
    "        # Example: ('e', 't'), which means that 'et' is a possible subword\n",
    "        # Each string pair is mapped to an unique index number\n",
    "        # (corresponding to their position in the self.vocab list)\n",
    "        self.merge_pairs = {}\n",
    "\n",
    "        # We add as basic vocab all characters of the extended ASCII\n",
    "        self.vocab = [chr(i) for i in range(256)]\n",
    "\n",
    "        with open(train_corpus_file, encoding='iso-8859-1') as fd: # I specified which encoding to use when reading the file\n",
    "\n",
    "            # We first read the corpus, split on white space, and count the\n",
    "            # occurrences of each distinct word\n",
    "            print(\"Counting word occurrences in corpus %s\"%train_corpus_file, end=\"...\", flush=True)\n",
    "            text = fd.read()\n",
    "            vocabulary_counts = {}\n",
    "            tokens = self.basic_tokenize(text)                     # Instead of using the split method, I applied my own function\n",
    "            for token in tokens:                     \n",
    "                vocabulary_counts[token] = vocabulary_counts.get(token, 0) + 1\n",
    "            print(\"Done\")\n",
    "\n",
    "            # We then iteratively extend the list of merge pairs until we\n",
    "            # reach the desired size. Note: to speed up the algorithm, we \n",
    "            # extract n merge pairs at each iteration\n",
    "            progress_bar = tqdm(total=vocab_size)\n",
    "            while len(self.vocab) < vocab_size:\n",
    "                most_common_pairs = self.get_most_common_pairs(vocabulary_counts)\n",
    "                for common_pair in most_common_pairs:\n",
    "                    self.merge_pairs[common_pair] = len(self.vocab)\n",
    "                    self.vocab.append(\"\".join(common_pair))\n",
    "                progress_bar.update(len(most_common_pairs))\n",
    "         #       print(\"Examples of new subwords:\", [\"\".join(pair) for pair in most_common_pairs][:10])\n",
    "            \n",
    "    def get_most_common_pairs(self, vocabulary_counts: Dict[str,int], \n",
    "                              n:int=200) -> List[Tuple[str,str]]:\n",
    "        \"\"\"Given a set of distinct words along with their corresponding number \n",
    "        of occurrences in the corpus, returns the n most frequent pairs of subwords.       \n",
    "        \"\"\"\n",
    "\n",
    "        # We count the frequencies of consecutive subwords in the vocabulary list\n",
    "        pair_freqs = {}\n",
    "        for word, word_count in vocabulary_counts.items():\n",
    "            subwords = self.tokenize_word(word)\n",
    "            for i in range(len(subwords)-1):\n",
    "                byte_pair = (subwords[i], subwords[i+1])\n",
    "                pair_freqs[byte_pair] = pair_freqs.get(byte_pair, 0) + word_count\n",
    "\n",
    "        # And return the most frequent ones\n",
    "        most_freq_pairs = sorted(pair_freqs.keys(), key=lambda x: pair_freqs[x])[::-1][:n]\n",
    "        return most_freq_pairs\n",
    "\n",
    "    def __call__(self, input:str, show_progress_bar=True) -> Iterator[str]:\n",
    "        \"\"\"Tokenizes a full text\"\"\"\n",
    "\n",
    "        # We first split into whitespace-separated tokens, and then in subwords\n",
    "        tokens = self.basic_tokenize(input)                              # Instead of using the split method, I applied my own function\n",
    "        for token in tqdm(tokens) if show_progress_bar else tokens:\n",
    "            subwords = self.tokenize_word(token)\n",
    "            for subword in subwords:\n",
    "                yield subword\n",
    "                \n",
    "    def tokenize_word(self, word):\n",
    "        \"\"\"Splits the word into subwords, according to the merge pairs \n",
    "        currently stored in self.merge_pairs.\"\"\"\n",
    "\n",
    "        # We start with a list of characters\n",
    "        # (+ a final character to denote the end of the word)    \n",
    "        splits = list(word) + [\" \"]\n",
    "\n",
    "        # We continue until there is nothing left to be merged\n",
    "        while len(splits)>=2:\n",
    "\n",
    "            # We extract consecutive subword pairs\n",
    "            pairs = [(splits[i], splits[i+1]) for i in range(len(splits)-1)]\n",
    "\n",
    "            # We find the \"best\" pair of subwords to merge -- that is, the one with the \n",
    "            # lowest position in the list of merge rules\n",
    "            best_pair_to_merge = min(pairs, key=lambda x: self.merge_pairs.get(x, np.inf))\n",
    "            if best_pair_to_merge in self.merge_pairs:\n",
    "\n",
    "                # We then merge the two subwords\n",
    "                for i in range(len(splits)-1):\n",
    "                    if (splits[i], splits[i+1]) == best_pair_to_merge:\n",
    "                        merged_subword = self.vocab[self.merge_pairs[best_pair_to_merge]]\n",
    "                        splits = splits[:i] + [merged_subword] + splits[i+2:]\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting word occurrences in corpus ndt_train_lm.txt...Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3c241e2c214a80999b6f23a6ca18e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdb8aa08c6840a5b1490718cba6ba2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259294 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 392625\n",
      "Number of types: 4316\n"
     ]
    }
   ],
   "source": [
    "# Testing the modified BPE Tokenizer\n",
    "\n",
    "# File paths for training and testing corpora\n",
    "train_file_path = 'ndt_train_lm.txt'\n",
    "test_file_path = 'ndt_test_lm.txt'\n",
    "\n",
    "# We train the tokenizer using the `ndt_train_lm.txt` corpus \n",
    "bpe_tokenizer = BPETokenizer_modified(train_corpus_file=train_file_path, vocab_size=5000)\n",
    "\n",
    "# We apply the BPE Tokenizer to the `ndt_test_lm.txt` corpus\n",
    "with open(test_file_path, mode='r', encoding='iso-8859-1') as test_file:\n",
    "    test_text = test_file.read()\n",
    "\n",
    "tokens = list(bpe_tokenizer(test_text))  # Tokenizing the test corpus using the __call__ method\n",
    "\n",
    "# We count the number of tokens and types\n",
    "num_tokens = len(tokens)             # Total number of tokens\n",
    "num_types = len(set(tokens))         # Number of unique tokens (types)\n",
    "\n",
    "print(f\"Number of tokens: {num_tokens}\")\n",
    "print(f\"Number of types: {num_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **BPE Tokenizer**\n",
    "\n",
    "Number of tokens: 386 753\n",
    "\n",
    "Number of types: 4 399\n",
    "\n",
    "- **Modified BPE Tokenizer**\n",
    "\n",
    "Number of tokens: 392 625\n",
    "\n",
    "Number of types: 4 316\n",
    "\n",
    "**Final considerations:**\n",
    "\n",
    "In the original tokenizer, we observe fewer tokens but more types. This could be because the modified BPE tokenizer splits punctuation signs, which increases the total number of tokens while simultaneously reducing the number of unique ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.5__ (_optional, 2 extra points_): In a [tweet](https://x.com/karpathy/status/1759996551378940395) published earlier this year, the well-known AI researcher Andrej Karpathy stressed that many of the current limitations of Large Language Models are actually a product of the tokenisation step. Explain at least 4 of the problems he mentioned in his tweet (you can of course search online, or watch Karpathy's own video lecture on tokenization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading Andrej Karpathy's tweet, I decided to focus on the first four issues LLM face as a result of tokenization. These particular issues are:\n",
    "- Why can't LLM spell words? **Tokenization.**\n",
    "- Why can't LLM do super simple string processing tasks like reversing a string? **Tokenization.**\n",
    "- Why is LLM worse at non-English languages (e.g. Japanese)? **Tokenization.**\n",
    "- Why is LLM bad at simple arithmetic? **Tokenization.**\n",
    "\n",
    "### Why can't LLM spell words?\n",
    "\n",
    "There are many examples on the Internet where users show how LLMs are unable to spell simple words or count the number of a particular letter in a particular word. The reason for this is that LLMs don't analyze words given as prompts letter by letter, but break them up into subwords (tokens) that may consist of the entire word or parts of the word itself. This shows that LLMs don't inherently understand how to manipulate individual letters the way we do. An example would be the handling of the word *'hello*, which could be split into *'hel' + 'lo*, making spelling corrections more difficult because the LLM is working at a higher level of abstraction (tokens), not directly at the character level.\n",
    "\n",
    "### Why can't LLM do super simple string processing tasks like reversing a string?\n",
    "\n",
    "Again, since LLMs operate at a higher level of abstraction represented by tokens, and not at the character level, if we give the LLM a particular word as a prompt and ask the model to invert it, the LLM might split the word into different tokens and then work on inverting each individual token, producing a final result that does not match the desired output. Using again the example of the word *'hello'*, split into the tokens *'hel' + 'lo'*, the LLM might first split the first token (*'hel* --> *'leh*), then the second one (*lo* --> *ol*), and in the worst case put them together to get *'lehol'*.\n",
    "\n",
    "### Why is LLM worse at non-English languages (e.g. Japanese)?\n",
    "\n",
    "The vast majority of tokenization algorithms were developed with English in mind and may not work well for all languages that have a different syntactic structure. For example, languages such as Japanese and Chinese don't have spaces between words, making the tokenization task more difficult due to the additional ambiguity that requires more context to understand the role each word plays. In addition, Japanese is characterized by the use of three different vocabularies, kanji, hiragana, and katakana, all of which occur simultaneously in different sentences, creating an even more complicated field for LLM to work in, given that it shouldn't mix the different vocabularies during the tokenization process.\n",
    "\n",
    "### Why is LLM bad at simple arithmetic?\n",
    "\n",
    "Given that LLM are trained to predict the next token in a sequence based on probabilities derived from massive amounts of text data, they don't actually associate the token represented by *'123'* with the actual value of 123, so when we ask the LLM to add another number (like 256) to that number, the model will base its answer solely on probabilistic models, without following the actual arithmetic rules behind addition. LLM do not treat numbers as actual numbers, they treat them as any other token.\n",
    "\n",
    "To avoid most of the above problems, it is possible to ask the LLM to solve the different requests using, for example, a Python script. The LLM is able to generate the correct syntax and structure for Python, based on patterns it has learned from large amounts of code data, allowing it to correctly handle operations such as string manipulation and different types of calculations, like addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: N-gram language models\n",
    "\n",
    "We will now train simple N-gram language models on the NDT corpus, using the tokenizers we have developed in Part 1.\n",
    "\n",
    "Here is the skeleton of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import abstractmethod\n",
    "\n",
    "class LanguageModel:\n",
    "    \"\"\"Generic class for running operations on language models, using a BPE tokenizer\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: BPETokenizer):\n",
    "        \"\"\"Build an abstract language model using the provided tokenizer\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    " \n",
    "    @abstractmethod\n",
    "    def predict(self, context_tokens: List[str]):\n",
    "        \"\"\"Given a list of context tokens (=previous tokens), returns a dictionary\n",
    "          mapping each possible token to its probability\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_perplexity(self, text: str):\n",
    "        \"\"\"Computes the perplexity of the given text according to the LM\"\"\"\n",
    "\n",
    "        print(\"Tokenising input text:\")\n",
    "        tokens = list(self.tokenizer(text))\n",
    "        \n",
    "        print(\"Computing perplexity:\")\n",
    "        log_probs = 0\n",
    "        for i in tqdm(range(len(tokens))):\n",
    "            context_tokens = [\"<s>\"] + tokens[:i]\n",
    "            predict_distrib = self.predict(context_tokens)\n",
    "\n",
    "            # We add the log-probabilities\n",
    "            log_probs += np.log(predict_distrib[tokens[i]])\n",
    "            \n",
    "        perplexity = np.exp(-log_probs/len(tokens))\n",
    "        return perplexity\n",
    "\n",
    "class NGramLanguageModel(LanguageModel):\n",
    "    \"\"\"Representation of a N-gram-based language model\"\"\"\n",
    "\n",
    "    def __init__(self, training_corpus_file: str, tokenizer:BPETokenizer, ngram_size:int=3,\n",
    "                  alpha_smoothing:float=1):\n",
    "        \"\"\"Initialize the N-gram model with:\n",
    "        - a file path to a training corpus to estimate the N-gram probabilities\n",
    "        - an already learned BPE tokenizer\n",
    "        - an N-gram size\n",
    "        - a smoothing parameter (Laplace smoothing)\"\"\"\n",
    "        \n",
    "        LanguageModel.__init__(self, tokenizer)\n",
    "        self.ngram_size = ngram_size\n",
    "        \n",
    "        # We define a simple backoff distribution (here just a uniform distribution)\n",
    "        self.default_distrib = {token:1/len(tokenizer.vocab) for token in tokenizer.vocab}\n",
    "\n",
    "        # Dictionary mapping a context (for instance the two preceding words if ngram_size=3)\n",
    "        # to another dictionary specifying the probability of each possible word in the \n",
    "        # vocabulary. The context should be a tuple of tokens.\n",
    "        self.ngram_probs = {}\n",
    "        with open(training_corpus_file) as fd:   \n",
    "\n",
    "            # based on the training corpus, tokenizer, ngram-size and smoothing parameter,\n",
    "            # fill the self.ngram_probs with the correct N-gram probabilities  \n",
    "            raise NotImplementedError()\n",
    " \n",
    "\n",
    "    def predict(self, context_tokens: List[str]):\n",
    "        \"\"\"Given a list of preceding tokens, returns the probability distribution \n",
    "        over the next possible token.\"\"\"\n",
    "\n",
    "        # We restrict the contextual tokens to (N-1) tokens\n",
    "        context_tokens = tuple(context_tokens[-self.ngram_size+1:])\n",
    "\n",
    "        # If the contextual tokens were indeed observed in the corpus, simply\n",
    "        # returns the precomputed probabilities\n",
    "        if context_tokens in self.ngram_probs:\n",
    "            return self.ngram_probs[context_tokens]\n",
    "        \n",
    "        # Otherwise, we return a uniform distribution over possible tokens\n",
    "        else:\n",
    "            return self.default_distrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.1__ (6 points): Complete the initialization method `__init__` to estimate the correct N-gram probabilities (with smoothing) based on the corpus. Don't worry about making your implementation super-efficient (although you can if you wish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, training_corpus_file: str, tokenizer: BPETokenizer, ngram_size: int = 2, alpha_smoothing: float = 1.0):\n",
    "    \"\"\"Initialize the N-gram model with:\n",
    "    - a file path to a training corpus to estimate the N-gram probabilities\n",
    "    - an already learned BPE tokenizer\n",
    "    - an N-gram size\n",
    "    - a smoothing parameter (Laplace smoothing)\"\"\"\n",
    "    # We initialize the parent class\n",
    "    LanguageModel.__init__(self, tokenizer)\n",
    "    self.ngram_size = ngram_size\n",
    "    self.alpha_smoothing = alpha_smoothing  # Store the smoothing parameter\n",
    "\n",
    "    # We build a new vocabulary including the special tokens to highlight the beginning and the end of a sentence\n",
    "    self.vocab = self.tokenizer.vocab.copy()    # We make a copy of the vocabulary \n",
    "                                                # from the BPE Tokenizer implemented before\n",
    "    self.vocab.append(\"<s>\")   # Start of sentence token\n",
    "    self.vocab.append(\"</s>\")  # End of sentence token\n",
    "    V = len(self.vocab)        # Vocabulary size\n",
    "\n",
    "    # Initialize dictionaries to hold counts\n",
    "    ngram_counts = {}     # Counts of n-grams\n",
    "    context_counts = {}   # Counts of contexts (n-1)-grams\n",
    "\n",
    "    with open(training_corpus_file, 'r', encoding='utf-8') as fd:\n",
    "\n",
    "        # Iterate over each line in the corpus\n",
    "        for line in fd:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # Skip empty lines\n",
    "\n",
    "            # Tokenize the line using the BPE tokenizer\n",
    "            tokens = list(self.tokenizer(line))\n",
    "\n",
    "            # Add start and end tokens to the list of tokens\n",
    "            tokens = [\"<s>\"] * (self.ngram_size - 1) + tokens + [\"</s>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.2__ (1 point): Train your language model in `ndt_train_lm.txt`, and compute its perplexity on the test data in `ndt_test_lm.txt`. The perplexity can be computed by calling the method `get_perplexity`. <br>\n",
    "(_Note_: if the training takes too much time, feel free to stop the process after looking at a fraction of the corpus, at least while you are testing/developing your training setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.3__ (_optional_, 4 bonus points): Improve the language model you have just developed. You can choose to focus on improving your model through a backoff mechanism, interpolation, or both. Once you are done, compute the perplexity again on the test corpus to ensure the language model has indeed improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text classification\n",
    "\n",
    "We will finally use the texts from the Norwegian Dependency Treebank for a classification task -- more precisely to determine whether a sentence is likely to be written in Bokmål or Nynorsk. To this end, we will use a simple bag-of-word setup (or more precisely a bag-of-_subwords_, since we will rely on the subwords extracted using BPE) along with a logistic regression model. As there is only two, mutually exclusive classes, you can view the task as a binary classification problem. \n",
    "\n",
    "The training data is found in `ndt_train_class.txt` and simply consists of a list of sentences, each sentence being mapped to a language form (Bokmål: `nob` or Nynorsk: `nno`). The language form is written at the end of each line, separated by a `\\t`. Note the training examples are currently _not_ shuffled.\n",
    "\n",
    "To train and apply your classifier, the easiest is to use the [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model from `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.1__ (2 points): Create a `N x V` matrix in which each line corresponds to a training example (out of `N` training instances) and each row corresponds to an individual feature, in this case the presence/absence of a particular subword in the sentence. In other words, there should be a total of `V` features, where `V` is the total size of the vocabulary for our BPE tokenizer. Also create a vector of length `N` with a value of `1` if the sentence was marked as Nynorsk, and 0 if is was marked as Bokmål. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 29905\n",
      "Target vector shape: (29905,)\n",
      "Number of features: 5056\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "\n",
    "# Reading the 'ndt_train_class.txt' file\n",
    "ndt_train_class_path = 'ndt_train_class.txt'\n",
    "\n",
    "with open(ndt_train_class_path, mode='r', encoding='utf-8') as file:\n",
    "    sentences_list = file.readlines()       # We use readlines() instead of the classic read()\n",
    "                                            # in order to obtain a list where each element is a line from the file\n",
    "\n",
    "sentences_list_without_labels = []          # A list containing sentences without the additional 'nno' and 'nob' labels\n",
    "labels = []                                 # 0 = Bokmål; 1 = Nynorsk\n",
    "\n",
    "for line in sentences_list:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue                            # We skip empty lines\n",
    "    if '\\t' in line:\n",
    "        sentence, label = line.split('\\t')  # The label is separated by the actual sentence by \\t\n",
    "                                            # so we are dividing the sentence from the label\n",
    "        if label == 'nno':\n",
    "            labels.append(1)                # Nynorsk\n",
    "        elif label == 'nob':\n",
    "            labels.append(0)                # Bokmål\n",
    "        else:\n",
    "            print(f\"Unknown label '{label}' found.\")\n",
    "            continue                        # Skip lines with unknown labels\n",
    "        sentences_list_without_labels.append(sentence)\n",
    "    else:\n",
    "        print(f\"Line format incorrect: '{line}'\")\n",
    "        continue                            # Skip lines without a tab character\n",
    "\n",
    "# ------------------- Finding the N value -------------------\n",
    "# Counting the number of different sentences in the file (N)\n",
    "N = len(sentences_list_without_labels)\n",
    "print(f'Number of sentences: {N}')\n",
    "\n",
    "# ------------------- Creating the target vector -------------------\n",
    "target_vector = np.array(labels)            # Vector of length N with a value of 1 if the sentence was marked as Nynorsk, \n",
    "                                            # and 0 if is was marked as Bokmål. \n",
    "print(f'Target vector shape: {target_vector.shape}')\n",
    "\n",
    "# ------------------- Finding the V value ------------------- \n",
    "vocabulary = bpe_tokenizer.vocab\n",
    "V = len(vocabulary)\n",
    "\n",
    "print(f'Number of features: {V}')\n",
    "\n",
    "# ------------------- Tokenizing each sentence ------------------- \n",
    "tokenized_sentences = []\n",
    "\n",
    "for sentence in sentences_list_without_labels:\n",
    "    tokens = list(bpe_tokenizer(sentence, show_progress_bar=False))\n",
    "    tokenized_sentences.append(tokens)\n",
    "\n",
    "# We nitialize an empty dictionary to keep track of the different subwords' indices\n",
    "subword_to_index = {}  \n",
    "\n",
    "# We loop through the vocabulary with both the index and the subword\n",
    "for idx, subword in enumerate(vocabulary):\n",
    "    subword_to_index[subword] = idx         # We assign the index to the subword\n",
    "\n",
    "# ------------------- Creating the feature matrix ------------------- \n",
    "X = np.zeros((N, V))\n",
    "\n",
    "for i, tokens in enumerate(tokenized_sentences):\n",
    "    for token in set(tokens):               # We use set to avoid duplicate subwords in a sentence\n",
    "        idx = subword_to_index.get(token)\n",
    "        if idx is not None:\n",
    "            X[i, idx] = 1                   # We mark the presence of the subword in the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.2__ (2 points): Use the data matrix you have just filled to train a logistic regression model (see the documentation on [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for more details). We recommend to use the `liblinear` solver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Fitting the model to our data (train data)\n",
    "model.fit(X, target_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.3__ (1 point): Now apply the learned logistic regression model to the test set in `ndt_test_class.txt`, and evaluate its performance in terms of accuracy, recall and precision (you can use the functionalities in `sklearn.metrics` to compute those easily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9458\n",
      "Recall: 0.9278\n",
      "Precision: 0.9557\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "# We read the 'ndt_test_class.txt' file\n",
    "ndt_test_class_path = 'ndt_test_class.txt'\n",
    "\n",
    "with open(ndt_test_class_path, mode='r', encoding='utf-8') as file:\n",
    "    test_sentences_list = file.readlines()\n",
    "\n",
    "test_sentences_list_without_labels = []\n",
    "test_labels = []\n",
    "\n",
    "for line in test_sentences_list:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    if '\\t' in line:\n",
    "        sentence, label = line.split('\\t')\n",
    "        if label == 'nno':\n",
    "            test_labels.append(1)  # Nynorsk\n",
    "        elif label == 'nob':\n",
    "            test_labels.append(0)  # Bokmål\n",
    "        else:\n",
    "            print(f\"Unknown label '{label}' found.\")\n",
    "            continue\n",
    "        test_sentences_list_without_labels.append(sentence)\n",
    "    else:\n",
    "        print(f\"Line format incorrect: '{line}'\")\n",
    "        continue\n",
    "\n",
    "# We convert the test labels to a numpy array\n",
    "test_target_vector = np.array(test_labels)\n",
    "\n",
    "# We tokenize the test sentences\n",
    "test_tokenized_sentences = []\n",
    "\n",
    "for sentence in test_sentences_list_without_labels:\n",
    "    tokens = list(bpe_tokenizer(sentence, show_progress_bar=False))\n",
    "    test_tokenized_sentences.append(tokens)\n",
    "\n",
    "# We create the feature matrix specifically for the test set\n",
    "X_test = np.zeros((len(test_sentences_list_without_labels), V))\n",
    "\n",
    "for i, tokens in enumerate(test_tokenized_sentences):\n",
    "    for token in set(tokens):\n",
    "        idx = subword_to_index.get(token)\n",
    "        if idx is not None:\n",
    "            X_test[i, idx] = 1\n",
    "\n",
    "# We create a variable containing the labels predictions for the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Finally, we evaluate the model's performance\n",
    "accuracy = accuracy_score(test_target_vector, predictions)\n",
    "recall = recall_score(test_target_vector, predictions)\n",
    "precision = precision_score(test_target_vector, predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'Precision: {precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.4__ (2 points): Inspect the weights learned by your logistic regression model (in `coef_`) and find the 5 subwords that contribute _the most_ to the classification of the sentence in Nynorsk. Also find the 5 subwords that contribute the most to the classification of the sentence in Bokmål. Do those weights make sense, according to what you know about Bokmål and Nynorsk ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 subwords contributing to Nynorsk classification:\n",
      "Subword: 'ikkje ', Coefficient: 4.3448\n",
      "Subword: 'eit ', Coefficient: 3.9601\n",
      "Subword: 'dei ', Coefficient: 3.6074\n",
      "Subword: 'ein ', Coefficient: 3.3617\n",
      "Subword: 'ane ', Coefficient: 3.0465\n",
      "\n",
      "Top 5 subwords contributing to Bokmål classification:\n",
      "Subword: 'ikke ', Coefficient: -3.2864\n",
      "Subword: 'sier ', Coefficient: -3.2536\n",
      "Subword: 'jeg ', Coefficient: -3.0967\n",
      "Subword: 'fra ', Coefficient: -2.8605\n",
      "Subword: 'Jeg ', Coefficient: -2.6171\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Accessing the coefficients -------------------\n",
    "# model.coef_ is a 2D array of shape (1, n_features),\n",
    "# we extract the coefficients as a 1D array\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# ------------------- Creating the index_to_subword dictionary -------------------\n",
    "# We reverse the subword_to_index dictionary previously created \n",
    "# to obtain an index_to_subword dictionary\n",
    "index_to_subword = {}\n",
    "for subword, idx in subword_to_index.items():\n",
    "    index_to_subword[idx] = subword\n",
    "\n",
    "# ------------------- Associating coefficients with subwords -------------------\n",
    "# We create a list of (subword, coefficient) tuples\n",
    "subword_coefficients = []\n",
    "for idx, coef in enumerate(coefficients):\n",
    "    subword = index_to_subword[idx]  # Get the subword corresponding to the index\n",
    "    subword_coefficients.append((subword, coef))\n",
    "\n",
    "# ------------------- Sorting the subwords based on coefficients -------------------\n",
    "# Sort subwords by coefficient values for positive contributions (descending)\n",
    "sorted_subwords_positive = sorted(subword_coefficients, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Sort subwords by coefficient values for negative contributions (ascending)\n",
    "sorted_subwords_negative = sorted(subword_coefficients, key=lambda x: x[1])\n",
    "\n",
    "# ------------------- Extracting and displaying the top 5 subwords -------------------\n",
    "# Top 5 subwords contributing to Nynorsk classification\n",
    "top_5_nynorsk = sorted_subwords_positive[:5]\n",
    "print(\"Top 5 subwords contributing to Nynorsk classification:\")\n",
    "for subword, coef in top_5_nynorsk:\n",
    "    print(f\"Subword: '{subword}', Coefficient: {coef:.4f}\")\n",
    "\n",
    "# Top 5 subwords contributing to Bokmål classification\n",
    "top_5_bokmal = sorted_subwords_negative[:5]\n",
    "print(\"\\nTop 5 subwords contributing to Bokmål classification:\")\n",
    "for subword, coef in top_5_bokmal:\n",
    "    print(f\"Subword: '{subword}', Coefficient: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final considerations:**\n",
    "\n",
    "Not being a native Norwegian speaker, it is a challenge for me to give a proper evaluation of the different subwords that contributed most to the classification model.\n",
    "\n",
    "However, it is immediately noticeable that the subword *'jeg'* (*'I'* in English) appears twice in the top 5 contributors to the Bokmal classification, and while this makes sense given that personal pronouns differ between the two standards, an explicit analysis of lower case only would have helped to avoid such repetition.\n",
    "\n",
    "In general, all of the subwords that most influenced the classification model appear to be quite characteristic of the standard to which they belong, which explains why they were so influential in the actual classification process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
